<p align="center">
  <img src="https://img.shields.io/badge/ğŸ¤–_Transformer-Explainer-7c63ff?style=for-the-badge&labelColor=1f2437" alt="Transformer Explainer" />
</p>

<h1 align="center">ğŸ§  Transformer Explainer</h1>
<h3 align="center">Interactive LLM Attention Simulation â€” GPT-2 Style</h3>

<p align="center">
  <em>Visualize how transformers think, one attention head at a time.</em>
</p>

<p align="center">
  <a href="https://simulasillm.vercel.app/">
    <img src="https://img.shields.io/badge/ğŸš€_Live_Demo-simulasillm.vercel.app-5b9dff?style=for-the-badge&labelColor=1f2437" alt="Live Demo" />
  </a>
  &nbsp;
  <a href="https://paper-llm-attention.vercel.app/">
    <img src="https://img.shields.io/badge/ğŸ“„_Research_Paper-Read_Now-2ec8a8?style=for-the-badge&labelColor=1f2437" alt="Research Paper" />
  </a>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/HTML5-E34F26?style=flat-square&logo=html5&logoColor=white" alt="HTML5" />
  <img src="https://img.shields.io/badge/CSS3-1572B6?style=flat-square&logo=css3&logoColor=white" alt="CSS3" />
  <img src="https://img.shields.io/badge/JavaScript-F7DF1E?style=flat-square&logo=javascript&logoColor=black" alt="JavaScript" />
  <img src="https://img.shields.io/badge/Vercel-000000?style=flat-square&logo=vercel&logoColor=white" alt="Vercel" />
  <img src="https://img.shields.io/badge/Zero_Dependencies-âœ…-brightgreen?style=flat-square" alt="Zero Dependencies" />
</p>

---

## ğŸ” Overview

An interactive, educational web simulation that lets you **see inside a transformer model** â€” from tokenization to next-token prediction. No black boxes. No abstract equations. Just a live, explorable pipeline modeled after **GPT-2 Small**.

> ğŸ’¡ Built for students, educators, and anyone curious about how Large Language Models actually work under the hood.

---

## âœ¨ Key Features

| Feature | Description |
|---------|-------------|
| ğŸ”¤ **Token Embedding** | Watch input text split into tokens and map to embedding vectors with positional encoding |
| ğŸ”‘ **Q/K/V Inspector** | Examine Query, Key, and Value projections for each attention head |
| ğŸ—ºï¸ **Attention Heatmap** | Interactive matrix with causal masking â€” see which tokens attend to which |
| ğŸ“Š **Probability Distribution** | Real-time softmax output showing candidate tokens and their probabilities |
| âš¡ **Autoregressive Generation** | Generate tokens step-by-step and observe how each new token reshapes attention |
| ğŸ›ï¸ **Sampling Controls** | Tune Temperature (0.3â€“1.5), Top-k (1â€“12), and generation length live |
| ğŸ‘ï¸ **Multi-Head View** | Switch between attention heads to compare learned patterns |

---

## ğŸ—ï¸ Architecture (Simulated)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GPT-2 Small (Simulation)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ§± Layers       â”‚  12                          â”‚
â”‚  ğŸ§  Attn Heads   â”‚  4 (visual)                  â”‚
â”‚  ğŸ“ Hidden Size  â”‚  16                          â”‚
â”‚  ğŸ”¢ Head Dim     â”‚  4                           â”‚
â”‚  ğŸ­ Causal Mask  â”‚  Active                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ› ï¸ Tech Stack

| | Technology | Purpose |
|---|-----------|---------|
| ğŸ“„ | **HTML5** | Semantic layout & SVG attention diagrams |
| ğŸ¨ | **CSS3** | Custom properties, Grid, Flexbox |
| âš™ï¸ | **Vanilla JS** | Simulation engine â€” zero dependencies |
| ğŸ”¤ | **Google Fonts** | Orbitron, Space Grotesk, IBM Plex Mono |
| ğŸŒ | **Vercel** | Static hosting & CDN |

> **No frameworks. No build step. No bundler.** Just clean, dependency-free code.

---

## ğŸ“ Project Structure

```
simulasiLLM/
â”œâ”€â”€ ğŸ“„ index.html        # UI layout â€” toolbar, attention SVG, token stream, panels
â”œâ”€â”€ ğŸ¨ style.css         # Styling with CSS custom properties
â”œâ”€â”€ âš™ï¸ app.js            # Engine â€” tokenizer, attention math, rendering, generation
â”œâ”€â”€ ğŸš€ DEPLOY.md         # Deployment guide (Vercel, Cloudflare, Netlify, GH Pages)
â””â”€â”€ ğŸ“¦ .github/
    â””â”€â”€ workflows/       # CI/CD configuration
```

---

## ğŸš€ Getting Started

### Run Locally

No install required â€” just serve the static files:

```bash
git clone https://github.com/romizone/simulasiLLM.git
cd simulasiLLM
python3 -m http.server 8081
```

Open [http://127.0.0.1:8081](http://127.0.0.1:8081) in your browser.

Alternatively:

```bash
# Node.js
npx serve .

# PHP
php -S localhost:8081
```

### Deploy

See [DEPLOY.md](./DEPLOY.md) for guides on Vercel, Cloudflare Pages, Netlify, and GitHub Pages.

---

## âš™ï¸ How It Works

```
  ğŸ“ Input Text
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ğŸ”¤ Token â”‚  Split text into tokens (Unicode-aware regex)
  â”‚   izer   â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ğŸ“ Embed â”‚  Map tokens â†’ dense vectors + positional encoding
  â”‚   ding   â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ğŸ”‘ Q/K/V â”‚  Project embeddings into Query, Key, Value spaces
  â”‚  Project â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ğŸ—ºï¸ Attn  â”‚  score = (Q Â· Káµ€) / âˆšd_k  â†’  causal mask  â†’  softmax
  â”‚  Matrix  â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ ğŸ² Sampleâ”‚  Apply temperature & top-k filtering
  â”‚          â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  âš¡ Next Token  â”€â”€â†’  (autoregressive loop)
```

---

## ğŸ“„ Research Paper

<table>
  <tr>
    <td>ğŸ“„</td>
    <td>
      <strong>"Simulating the Attention Mechanism in Large Language Models Based on the GPT-2 Architecture"</strong><br/><br/>
      <a href="https://paper-llm-attention.vercel.app/">ğŸ“– Read the full paper â†’</a>
    </td>
  </tr>
</table>

**Topics covered:**

- ğŸ”¤ Token processing pipeline (BPE tokenization, embedding, positional encoding)
- ğŸ“ Mathematical formulation of scaled dot-product attention
- ğŸ­ Causal masking in autoregressive generation
- ğŸŒ¡ï¸ Temperature scaling and top-k sampling strategies
- ğŸ—ï¸ GPT-2 Small architecture specifications

---

## ğŸ”— Links

| | Link |
|---|------|
| ğŸš€ | **Live Simulation** â€” [simulasillm.vercel.app](https://simulasillm.vercel.app/) |
| ğŸ“„ | **Research Paper** â€” [paper-llm-attention.vercel.app](https://paper-llm-attention.vercel.app/) |
| ğŸ’» | **Source Code** â€” [github.com/romizone/simulasiLLM](https://github.com/romizone/simulasiLLM) |

---

## ğŸ“œ License

This project is open source and available for educational purposes.

---

<p align="center">
  <strong>Made with â¤ï¸ by <a href="https://github.com/romizone">Romi Nur Ismanto</a></strong>
</p>
